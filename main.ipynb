{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import csv\n",
    "from tqdm import tqdm\n",
    "from sklearn.model_selection import train_test_split\n",
    "import math\n",
    "from sklearn.cluster import KMeans\n",
    "import matplotlib.pyplot\n",
    "from matplotlib import pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getHumanObservedData_From_Files():\n",
    "    features_df = pd.read_csv('HumanObserved-Dataset/HumanObserved-Features-Data/HumanObserved-Features-Data.csv')\n",
    "    diffn_pairs_df = pd.read_csv('HumanObserved-Dataset/HumanObserved-Features-Data/diffn_pairs.csv')    \n",
    "    same_pairs_df = pd.read_csv('HumanObserved-Dataset/HumanObserved-Features-Data/same_pairs.csv')\n",
    "    features_df.drop(['Unnamed: 0'],axis=1,inplace=True)\n",
    "    return features_df, same_pairs_df, diffn_pairs_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getGSCData_From_Files():\n",
    "    features_df = pd.read_csv('GSC-Dataset/GSC-Features-Data/GSC-Features.csv')\n",
    "    diffn_pairs_df = pd.read_csv('GSC-Dataset/GSC-Features-Data/diffn_pairs.csv')\n",
    "    same_pairs_df = pd.read_csv('GSC-Dataset/GSC-Features-Data/same_pairs.csv')\n",
    "    return features_df, same_pairs_df, diffn_pairs_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getHumanObservedData_Concatenated():\n",
    "    features_df, same_pairs_df, diffn_pairs_df = getHumanObservedData_From_Files()    \n",
    "    concat_same_df = pd.DataFrame()\n",
    "\n",
    "    concat_same_df['img_id_A']=same_pairs_df.img_id_A\n",
    "    concat_same_df['img_id_B']=same_pairs_df.img_id_B\n",
    "    concat_same_df=pd.merge(concat_same_df,features_df,left_on='img_id_A', right_on='img_id')\n",
    "    concat_same_df=pd.merge(concat_same_df,features_df,left_on='img_id_B', right_on='img_id',suffixes=('_a', '_b'))\n",
    "    concat_same_df.drop(['img_id_a','img_id_b'],axis=1,inplace=True)\n",
    "    #concat_same_df=pd.merge(concat_same_df,same_pairs_df)\n",
    "    concat_same_df['target']=same_pairs_df.target\n",
    "    \n",
    "    concat_diffn_df=pd.DataFrame()\n",
    "\n",
    "    concat_diffn_df['img_id_A']=diffn_pairs_df.img_id_A\n",
    "    concat_diffn_df['img_id_B']=diffn_pairs_df.img_id_B\n",
    "    concat_diffn_df=pd.merge(concat_diffn_df,features_df,left_on='img_id_A', right_on='img_id')\n",
    "    concat_diffn_df=pd.merge(concat_diffn_df,features_df,left_on='img_id_B', right_on='img_id',suffixes=('_a', '_b'))\n",
    "    concat_diffn_df.drop(['img_id_a','img_id_b'],axis=1,inplace=True)\n",
    "    #concat_diffn_df=pd.merge(concat_diffn_df,diffn_pairs_df)\n",
    "    concat_diffn_df['target'] = diffn_pairs_df.target\n",
    "\n",
    "    \n",
    "    concat_full_df=concat_same_df.append(concat_diffn_df.iloc[0:800,:])\n",
    "    concat_full_df = concat_full_df.sample(frac=1).reset_index(drop=True)\n",
    "    \n",
    "    X_train_and_cv_concat, X_test_concat, y_train_and_cv_concat, y_test_concat = train_test_split(concat_full_df.iloc[:,2:-1], concat_full_df['target'].values, test_size=0.2)\n",
    "    X_train_concat, X_cv_concat, y_train_concat, y_cv_concat = train_test_split(X_train_and_cv_concat, y_train_and_cv_concat, test_size=0.25)\n",
    "\n",
    "    return X_train_concat, y_train_concat,  X_cv_concat, y_cv_concat, X_test_concat, y_test_concat\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getHumanObservedData_Subtracted():\n",
    "    features_df, same_pairs_df, diffn_pairs_df = getHumanObservedData_From_Files()   \n",
    "    subtraction_same_df=pd.DataFrame()\n",
    "\n",
    "    subtraction_same_df['img_id_A']=same_pairs_df.img_id_A\n",
    "    subtraction_same_df['img_id_B']=same_pairs_df.img_id_B\n",
    "    subtraction_same_df=pd.merge(subtraction_same_df,features_df,left_on='img_id_A', right_on='img_id')\n",
    "    subtraction_same_df.drop(['img_id'],axis=1,inplace=True)\n",
    "    subtraction_same_df.iloc[:,1:].subtract(features_df.iloc[:,1:])\n",
    "    subtraction_same_df=pd.merge(subtraction_same_df,same_pairs_df)\n",
    "    \n",
    "    subtraction_diffn_df=pd.DataFrame()\n",
    "\n",
    "    subtraction_diffn_df['img_id_A']=diffn_pairs_df.img_id_A\n",
    "    subtraction_diffn_df['img_id_B']=diffn_pairs_df.img_id_B\n",
    "    subtraction_diffn_df=pd.merge(subtraction_diffn_df,features_df,left_on='img_id_A', right_on='img_id')\n",
    "    subtraction_diffn_df.drop(['img_id'],axis=1,inplace=True)\n",
    "    subtraction_diffn_df.iloc[:,1:].subtract(features_df.iloc[:,1:])\n",
    "    subtraction_diffn_df=pd.merge(subtraction_diffn_df,diffn_pairs_df)\n",
    "    \n",
    "    subtraction_full_df=subtraction_same_df.append(subtraction_diffn_df.iloc[0:800,:])\n",
    "    subtraction_full_df = subtraction_full_df.sample(frac=1).reset_index(drop=True)\n",
    "    \n",
    "    X_train_and_cv_subtract, X_test_subtract, y_train_and_cv_subtract, y_test_subtract = train_test_split(subtraction_full_df.iloc[:,2:-1], subtraction_full_df['target'].values, test_size=0.2)\n",
    "    X_train_subtract, X_cv_subtract, y_train_subtract, y_cv_subtract = train_test_split(X_train_and_cv_subtract, y_train_and_cv_subtract, test_size=0.25)\n",
    "\n",
    "    return X_train_subtract,  y_train_subtract, X_cv_subtract, y_cv_subtract, X_test_subtract, y_test_subtract"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getGSCData_Concatenated():\n",
    "    features_df, same_pairs_df, diffn_pairs_df = getGSCData_From_Files()  \n",
    "    concat_same_df=pd.DataFrame()\n",
    "\n",
    "    concat_same_df['img_id_A']=same_pairs_df.img_id_A\n",
    "    concat_same_df['img_id_B']=same_pairs_df.img_id_B\n",
    "    concat_same_df=pd.merge(concat_same_df,features_df,left_on='img_id_A', right_on='img_id')\n",
    "    concat_same_df=pd.merge(concat_same_df,features_df,left_on='img_id_B', right_on='img_id',suffixes=('_a', '_b'))\n",
    "    concat_same_df.drop(['img_id_a','img_id_b'],axis=1,inplace=True)\n",
    "    concat_same_df=pd.merge(concat_same_df,same_pairs_df)\n",
    "    \n",
    "    concat_diffn_df=pd.DataFrame()\n",
    "\n",
    "    concat_diffn_df['img_id_A']=diffn_pairs_df.img_id_A\n",
    "    concat_diffn_df['img_id_B']=diffn_pairs_df.img_id_B\n",
    "    concat_diffn_df=pd.merge(concat_diffn_df,features_df,left_on='img_id_A', right_on='img_id')\n",
    "    concat_diffn_df=pd.merge(concat_diffn_df,features_df,left_on='img_id_B', right_on='img_id',suffixes=('_a', '_b'))\n",
    "    concat_diffn_df.drop(['img_id_a','img_id_b'],axis=1,inplace=True)\n",
    "    concat_diffn_df=pd.merge(concat_diffn_df,diffn_pairs_df)\n",
    "    \n",
    "    concat_full_df=concat_same_df.append(concat_diffn_df.iloc[0:72000,:])\n",
    "    \n",
    "    concat_full_df = concat_full_df.sample(frac=1).reset_index(drop=True)\n",
    "    \n",
    "    X_train_and_cv_concat, X_test_concat, y_train_and_cv_concat, y_test_concat = train_test_split(concat_full_df.iloc[:,2:-1], concat_full_df['target'].values, test_size=0.2)\n",
    "    X_train_concat, X_cv_concat, y_train_concat, y_cv_concat = train_test_split(X_train_and_cv_concat, y_train_and_cv_concat, test_size=0.25)\n",
    "\n",
    "    return X_train_concat, y_train_concat, X_cv_concat, y_cv_concat, X_test_concat, y_test_concat\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getGSCData_Subtracted():\n",
    "    features_df, same_pairs_df, diffn_pairs_df = getGSCData_From_Files() \n",
    "    subtraction_same_df=pd.DataFrame()\n",
    "\n",
    "    subtraction_same_df['img_id_A']=same_pairs_df.img_id_A\n",
    "    subtraction_same_df['img_id_B']=same_pairs_df.img_id_B\n",
    "    subtraction_same_df=pd.merge(subtraction_same_df,features_df,left_on='img_id_A', right_on='img_id')\n",
    "    subtraction_same_df.drop(['img_id'],axis=1,inplace=True)\n",
    "    subtraction_same_df.iloc[:,1:].subtract(features_df.iloc[:,1:])\n",
    "    subtraction_same_df=pd.merge(subtraction_same_df,same_pairs_df)\n",
    "\n",
    "\n",
    "    subtraction_diffn_df=pd.DataFrame()\n",
    "\n",
    "    subtraction_diffn_df['img_id_A']=diffn_pairs_df.img_id_A\n",
    "    subtraction_diffn_df['img_id_B']=diffn_pairs_df.img_id_B\n",
    "    subtraction_diffn_df=pd.merge(subtraction_diffn_df,features_df,left_on='img_id_A', right_on='img_id')\n",
    "    subtraction_diffn_df.drop(['img_id'],axis=1,inplace=True)\n",
    "    subtraction_diffn_df.iloc[:,1:].subtract(features_df.iloc[:,1:])\n",
    "    subtraction_diffn_df=pd.merge(subtraction_diffn_df,diffn_pairs_df)\n",
    "\n",
    "    subtraction_full_df = subtraction_same_df.append(subtraction_diffn_df.iloc[0:72000,:])\n",
    "    subtraction_full_df = subtraction_full_df.sample(frac=1).reset_index(drop=True)\n",
    "    \n",
    "    X_train_and_cv_subtract, X_test_subtract, y_train_and_cv_subtract, y_test_subtract = train_test_split(subtraction_full_df.iloc[:,2:-1], subtraction_full_df['target'].values, test_size=0.2)\n",
    "    X_train_subtract, X_cv_subtract, y_train_subtract, y_cv_subtract = train_test_split(X_train_and_cv_subtract, y_train_and_cv_subtract, test_size=0.25)\n",
    "    \n",
    "    return X_train_subtract,  y_train_subtract, X_cv_subtract, y_cv_subtract, X_test_subtract, y_test_subtract    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "def insert_intercept(X):\n",
    "    intercept = np.ones((X.shape[0],1))\n",
    "    return np.concatenate((intercept,X),axis=1)\n",
    "\n",
    "def initialize_weights(X):\n",
    "    w = np.zeros(X.shape[1])\n",
    "    return w\n",
    "\n",
    "def sigmoid(z):\n",
    "    return 1 /(1 + np.exp(-z))\n",
    "\n",
    "def logistic_loss(h, y):\n",
    "        return (-y * np.log(h) - (1 - y) * np.log(1 - h)).mean()\n",
    "    \n",
    "def GetErms(VAL_TEST_OUT,ValDataAct):\n",
    "    sum = 0.0\n",
    "    t=0\n",
    "    accuracy = 0.0\n",
    "    counter = 0\n",
    "    val = 0.0\n",
    "    for i in range (0,len(VAL_TEST_OUT)):\n",
    "        sum = sum + math.pow((ValDataAct[i] - VAL_TEST_OUT[i]),2)\n",
    "        if(int(np.around(VAL_TEST_OUT[i], 0)) == ValDataAct[i]):\n",
    "            counter+=1\n",
    "    accuracy = (float((counter*100))/float(len(VAL_TEST_OUT)))\n",
    "    return (str(accuracy) + ',' +  str(math.sqrt(sum/len(VAL_TEST_OUT))))\n",
    "\n",
    "def predict(h, threshold):\n",
    "    return (h >= threshold)\n",
    "    \n",
    "def accuracy(predicted,actual):\n",
    "    correct = np.ones(predicted.shape[0])[predicted==actual]\n",
    "    return (correct.sum()/predicted.shape[0])\n",
    "\n",
    "def gradient_descent(X,y,w,alpha=0.01,reg_param=0,iterations=10000):\n",
    "    loss=[]\n",
    "    erms=[]\n",
    "    for i in tqdm(range(0,iterations)):\n",
    "        z=np.dot(X,w)       \n",
    "        diff_vector = sigmoid(z) - y\n",
    "        delta_E = np.dot(X.T,diff_vector)/X.shape[0]        \n",
    "        reg_term = np.dot(reg_param,w[1:])\n",
    "        update_term_0 = -np.dot((alpha/X.shape[0]),delta_E[0])\n",
    "        update_term = -np.dot((alpha/X.shape[0]),np.add(delta_E[1:],reg_term))\n",
    "        w[0]=np.add(w[0],update_term[0])\n",
    "        w[1:]=np.add(w[1:],update_term)        \n",
    "        update_term = alpha*delta_E\n",
    "        w-=update_term\n",
    "    return w\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "def LogisticRegression(X,y,learning_rate=0.01,la=0,iterate=1000000):\n",
    "    w = initialize_weights(X)\n",
    "    W_trained = gradient_descent(X,y,w,alpha=learning_rate,reg_param=la,iterations=iterate)\n",
    "    return W_trained\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_logisticRegression_metrics(X,y,W_trained):\n",
    "    h = sigmoid(np.dot(X,W_trained))  # Replace W_Now by W_concat_trained\n",
    "    predictions = predict(h, 0.5)\n",
    "    erms = GetErms(h,y).split(',')[1]\n",
    "    loss_logistic = logistic_loss(h,y)\n",
    "    acc = accuracy(predictions,y)\n",
    "    return erms,loss_logistic,acc\n",
    "\n",
    "def GetErms(VAL_TEST_OUT,ValDataAct):\n",
    "    sum = 0.0\n",
    "    t=0\n",
    "    accuracy = 0.0\n",
    "    counter = 0\n",
    "    val = 0.0 \n",
    "    for i in range (0,len(VAL_TEST_OUT)):\n",
    "        sum = sum + math.pow((ValDataAct[i] - VAL_TEST_OUT[i]),2)\n",
    "        if(int(np.around(VAL_TEST_OUT[i], 0)) == ValDataAct[i]):\n",
    "            counter+=1\n",
    "    accuracy = (float((counter*100))/float(len(VAL_TEST_OUT)))\n",
    "    ##print (\"Accuracy Generated..\")\n",
    "    ##print (\"Validation E_RMS : \" + str(math.sqrt(sum/len(VAL_TEST_OUT))))\n",
    "    return (str(accuracy) + ',' +  str(math.sqrt(sum/len(VAL_TEST_OUT))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Human Observed Data Set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Logistic Regression Model:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Concatenated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_hod_con,y_train_hod_con,X_cv_hod_con,y_cv_hod_con,X_test_hod_con,y_test_hod_con = getHumanObservedData_Concatenated()\n",
    "\n",
    "X_train_hod_con = insert_intercept(X_train_hod_con)\n",
    "X_cv_hod_con = insert_intercept(X_cv_hod_con)\n",
    "X_test_hod_con = insert_intercept(X_test_hod_con)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1000000/1000000 [00:43<00:00, 23182.50it/s]\n"
     ]
    }
   ],
   "source": [
    "W_trained_hod_con = LogisticRegression(X_train_hod_con,y_train_hod_con)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Regression Loss On Training Data: 0.36788413820929255\n",
      "ERMS Loss On Training Data: 0.33806033157156323\n",
      "Accuracy On Training Data: 0.8542976939203354\n",
      "Loss On CV Data: 0.33400884972241557\n",
      "Accuracy On CV Data: 0.8710691823899371\n",
      "Loss On Test Data: 0.31899720154649436\n",
      "Accuracy On Test Data: 0.890282131661442\n"
     ]
    }
   ],
   "source": [
    "train_loss,train_loss_logistic,train_accuracy = get_logisticRegression_metrics(X_train_hod_con,y_train_hod_con,W_trained_hod_con)\n",
    "cv_loss,cv_loss_logistic,cv_accuracy = get_logisticRegression_metrics(X_cv_hod_con,y_cv_hod_con,W_trained_hod_con)\n",
    "test_loss,test_loss_logistic,test_accuracy = get_logisticRegression_metrics(X_test_hod_con,y_test_hod_con,W_trained_hod_con)\n",
    "\n",
    "print(f'Logistic Regression Loss On Training Data: {train_loss_logistic}')\n",
    "print(f'ERMS Loss On Training Data: {train_loss}\\nAccuracy On Training Data: {train_accuracy}')\n",
    "print(f'Loss On CV Data: {cv_loss}\\nAccuracy On CV Data: {cv_accuracy}')\n",
    "print(f'Loss On Test Data: {test_loss}\\nAccuracy On Test Data: {test_accuracy}')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Subtracted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_hod_sub,y_train_hod_sub,X_cv_hod_sub,y_cv_hod_sub,X_test_hod_sub,y_test_hod_sub = getHumanObservedData_Subtracted()\n",
    "\n",
    "X_train_hod_sub = insert_intercept(X_train_hod_sub)\n",
    "X_cv_hod_sub  = insert_intercept(X_cv_hod_sub)\n",
    "X_test_hod_sub  = insert_intercept(X_test_hod_sub)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1000000/1000000 [00:38<00:00, 25949.38it/s]\n"
     ]
    }
   ],
   "source": [
    "W_trained_hod_sub = LogisticRegression(X_train_hod_sub,y_train_hod_sub)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Regression Loss On Training Data: nan\n",
      "Loss On Training Data: 0.21913370043476285\n",
      "Accuracy On Training Data: 0.9433962264150944\n",
      "Loss On CV Data: 0.20955312505760454\n",
      "Accuracy On CV Data: 0.949685534591195\n",
      "Loss On Test Data: 0.20193281019609147\n",
      "Accuracy On Test Data: 0.9498432601880877\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:13: RuntimeWarning: divide by zero encountered in log\n",
      "  del sys.path[0]\n",
      "/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:13: RuntimeWarning: invalid value encountered in multiply\n",
      "  del sys.path[0]\n"
     ]
    }
   ],
   "source": [
    "train_loss,train_loss_logistic,train_accuracy = get_logisticRegression_metrics(X_train_hod_sub,y_train_hod_sub,W_trained_hod_sub)\n",
    "cv_loss,cv_loss_logistic,cv_accuracy = get_logisticRegression_metrics(X_cv_hod_sub,y_cv_hod_sub,W_trained_hod_sub)\n",
    "test_loss,test_loss_logistic,test_accuracy = get_logisticRegression_metrics(X_test_hod_sub,y_test_hod_sub,W_trained_hod_sub)\n",
    "\n",
    "print(f'Logistic Regression Loss On Training Data: {train_loss_logistic}')\n",
    "print(f'Loss On Training Data: {train_loss}\\nAccuracy On Training Data: {train_accuracy}')\n",
    "print(f'Loss On CV Data: {cv_loss}\\nAccuracy On CV Data: {cv_accuracy}')\n",
    "print(f'Loss On Test Data: {test_loss}\\nAccuracy On Test Data: {test_accuracy}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Linear Regression Model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "def GenerateBigSigma(Data, MuMatrix,TrainingPercent,IsSynthetic=False):\n",
    "    BigSigma    = np.zeros((len(Data),len(Data)))\n",
    "    DataT       = np.transpose(Data)\n",
    "    TrainingLen = math.ceil(len(DataT)*(TrainingPercent*0.01))        \n",
    "    varVect     = []\n",
    "    for i in range(0,len(DataT[0])):\n",
    "        vct = []\n",
    "        for j in range(0,int(TrainingLen)):\n",
    "            vct.append(Data[i][j])    \n",
    "        varVect.append(np.var(vct))\n",
    "    \n",
    "    for j in range(len(Data)):\n",
    "        BigSigma[j][j] = varVect[j]\n",
    "    if IsSynthetic == True:\n",
    "        BigSigma = np.dot(3,BigSigma)\n",
    "    else:\n",
    "        BigSigma = np.dot(200,BigSigma)\n",
    "    ##print (\"BigSigma Generated..\")\n",
    "    return BigSigma\n",
    "\n",
    "def GetScalar(DataRow,MuRow, BigSigInv):  \n",
    "    R = np.subtract(DataRow,MuRow)\n",
    "    T = np.dot(BigSigInv,np.transpose(R))  \n",
    "    L = np.dot(R,T)\n",
    "    return L\n",
    "\n",
    "def GetRadialBasisOut(DataRow,MuRow, BigSigInv):    \n",
    "    phi_x = math.exp(-0.5*GetScalar(DataRow,MuRow,BigSigInv))\n",
    "    return phi_x\n",
    "\n",
    "def GetPhiMatrix(Data, MuMatrix, BigSigma, TrainingPercent = 100):\n",
    "    DataT = np.transpose(Data)\n",
    "    TrainingLen = math.ceil(len(DataT)*(TrainingPercent*0.01))         \n",
    "    PHI = np.zeros((int(TrainingLen),len(MuMatrix))) \n",
    "    BigSigInv = np.linalg.pinv(BigSigma)\n",
    "    for  C in range(0,len(MuMatrix)):\n",
    "        for R in range(0,int(TrainingLen)):\n",
    "            PHI[R][C] = GetRadialBasisOut(DataT[R], MuMatrix[C], BigSigInv)\n",
    "    #print (\"PHI Generated..\")\n",
    "    return PHI\n",
    "\n",
    "def GetValTest(VAL_PHI,W):\n",
    "    Y = np.dot(W,np.transpose(VAL_PHI))\n",
    "    ##print (\"Test Out Generated..\")\n",
    "    return Y\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradient_descent_lin_reg(TRAINING_PHI,y,m,alpha=0.01,reg_param=2,iterations=10000):\n",
    "\n",
    "    W_Now        = np.zeros((m))\n",
    "    La           = reg_param\n",
    "    learningRate = alpha\n",
    "    print(TRAINING_PHI.shape)\n",
    "    for i in tqdm(range(0,iterations)):\n",
    "        #print ('---------Iteration: ' + str(i) + '--------------') \n",
    "        Delta_E_D     = -np.dot(TRAINING_PHI.T,(y - np.dot(TRAINING_PHI,W_Now))) # Computes the vectorised form of partial derivative of cost function with respect to the weights.  \n",
    "        La_Delta_E_W  = np.dot(La,W_Now) #Computes the partial derivate of regularization term\n",
    "        Delta_E       = np.add(Delta_E_D,La_Delta_E_W)  # Sums up above two derivatives to give the complete derivate of a regularized cost function.\n",
    "        Delta_W       = -np.dot(learningRate,Delta_E) # Multiplies the derivative with learning rate\n",
    "        W_T_Next      = W_Now + (Delta_W/y.shape[0]) #Updates the weights by adding the above calculated derivative.\n",
    "        W_Now         = W_T_Next\n",
    "    return W_Now"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_metrics_lin_reg(PHI,y,W,name):\n",
    "    Prediction    = GetValTest(PHI,W) \n",
    "    Erms          = GetErms(Prediction,y)\n",
    "    L_Erms        = float(Erms.split(',')[1])\n",
    "    print(\"E_rms \" + name + \": \" + str(np.around(L_Erms,5)))\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "def LinearRegression(X_train,y_train,X_cv,y_cv,X_test,y_test,m=10,learning_rate=0.01,la=2,iterate=1000000):\n",
    "    \n",
    "    kmeans = KMeans(n_clusters=m, random_state=0).fit(X_train) # Cluster the data to create centroids (mean values) which will be used to compute gaussian kernals \n",
    "    Mu = kmeans.cluster_centers_ #matrix containing the mean values of features for every data point in each cluster\n",
    "    print(Mu.shape)\n",
    "    BigSigma     = GenerateBigSigma(X_train.T, Mu, 100) # compute the covariance Matrix\n",
    "    TRAINING_PHI = GetPhiMatrix(X_train.T, Mu, BigSigma, 100) # Compute the Gaussian radial basis for training data.\n",
    "    TEST_PHI     = GetPhiMatrix(X_test.T, Mu, BigSigma, 100) \n",
    "    VAL_PHI      = GetPhiMatrix(X_cv.T, Mu, BigSigma, 100)\n",
    "    W_trained    = gradient_descent_lin_reg(TRAINING_PHI,y_train,m,alpha=learning_rate,reg_param=la,iterations=iterate)\n",
    "   \n",
    "    train_pred = np.dot(TRAINING_PHI,W_trained)\n",
    "    cv_pred = np.dot(VAL_PHI,W_trained)\n",
    "    test_pred = np.dot(TEST_PHI,W_trained)\n",
    "    \n",
    "    print(\"Train Accuracy: \"+str(accuracy(train_pred>=0.5,y_train)))\n",
    "    print(\"CV Accuracy: \"+str(accuracy(cv_pred>=0.5,y_cv)))\n",
    "    print(\"Test Accuracy: \"+str(accuracy(test_pred>=0.5,y_test)))\n",
    "             \n",
    "    print_metrics_lin_reg(TRAINING_PHI,y_train,W_trained,\"TRAINING\")\n",
    "    print_metrics_lin_reg(VAL_PHI,y_cv,W_trained,\"VALIDATION\")\n",
    "    print_metrics_lin_reg(TEST_PHI,y_test,W_trained,\"TEST\")\n",
    "    return W_trained"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Concatenate Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 3605/1000000 [00:00<00:27, 36042.82it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10, 18)\n",
      "(954, 10)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1000000/1000000 [00:22<00:00, 43656.82it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Accuracy: 0.6750524109014675\n",
      "CV Accuracy: 0.6383647798742138\n",
      "Test Accuracy: 0.7053291536050157\n",
      "E_rms TRAINING: 0.49051\n",
      "E_rms VALIDATION: 0.49174\n",
      "E_rms TEST: 0.48871\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "w_c = LinearRegression(X_train_hod_con[:,1:],y_train_hod_con,X_cv_hod_con[:,1:],y_cv_hod_con,X_test_hod_con[:,1:],y_test_hod_con)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Subtracted Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|          | 0/1000000 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10, 9)\n",
      "(954, 10)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1000000/1000000 [00:22<00:00, 44529.68it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Accuracy: 0.8427672955974843\n",
      "CV Accuracy: 0.8113207547169812\n",
      "Test Accuracy: 0.8369905956112853\n",
      "E_rms TRAINING: 0.46094\n",
      "E_rms VALIDATION: 0.46586\n",
      "E_rms TEST: 0.46029\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "w_s = LinearRegression(X_train_hod_sub[:,1:],y_train_hod_sub,X_cv_hod_sub[:,1:],y_cv_hod_sub,X_test_hod_sub[:,1:],y_test_hod_sub)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GSC Data Set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Logistic Regression Model:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Concatenated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_gsc_con,y_train_gsc_con,X_cv_gsc_con,y_cv_gsc_con,X_test_gsc_con,y_test_gsc_con = getGSCData_Concatenated()\n",
    "\n",
    "X_train_gsc_con = insert_intercept(X_train_gsc_con)\n",
    "X_cv_gsc_con = insert_intercept(X_cv_gsc_con)\n",
    "X_test_gsc_con = insert_intercept(X_test_gsc_con)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10000/10000 [33:54<00:00, 17.15it/s]    \n"
     ]
    }
   ],
   "source": [
    "W_trained_gsc_con = LogisticRegression(X_train_gsc_con,y_train_gsc_con,iterate=10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Regression Loss On Training Data: 0.35762210364924907\n",
      "Loss On Training Data: 0.3275012269540261\n",
      "Accuracy On Training Data: 0.8722682830534848\n",
      "Loss On CV Data: 0.3305439936666008\n",
      "Accuracy On CV Data: 0.8683898836480178\n",
      "Loss On Test Data: 0.3290761184026051\n",
      "Accuracy On Test Data: 0.8703800466785104\n"
     ]
    }
   ],
   "source": [
    "train_loss, train_loss_logistic,train_accuracy = get_logisticRegression_metrics(X_train_gsc_con,y_train_gsc_con,W_trained_gsc_con)\n",
    "cv_loss,cv_loss_logistic,cv_accuracy = get_logisticRegression_metrics(X_cv_gsc_con,y_cv_gsc_con,W_trained_gsc_con)\n",
    "test_loss,test_loss_logistic,test_accuracy = get_logisticRegression_metrics(X_test_gsc_con,y_test_gsc_con,W_trained_gsc_con)\n",
    "\n",
    "print(f'Logistic Regression Loss On Training Data: {train_loss_logistic}')\n",
    "print(f'Loss On Training Data: {train_loss}\\nAccuracy On Training Data: {train_accuracy}')\n",
    "print(f'Loss On CV Data: {cv_loss}\\nAccuracy On CV Data: {cv_accuracy}')\n",
    "print(f'Loss On Test Data: {test_loss}\\nAccuracy On Test Data: {test_accuracy}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Subtracted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_gsc_sub,y_train_gsc_sub,X_cv_gsc_sub,y_cv_gsc_sub,X_test_gsc_sub,y_test_gsc_sub = getGSCData_Subtracted()\n",
    "\n",
    "X_train_gsc_sub = insert_intercept(X_train_gsc_sub)\n",
    "X_cv_gsc_sub  = insert_intercept(X_cv_gsc_sub)\n",
    "X_test_gsc_sub  = insert_intercept(X_test_gsc_sub)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1., 0., 1., ..., 0., 0., 0.],\n",
       "       [1., 0., 0., ..., 0., 0., 0.],\n",
       "       [1., 1., 1., ..., 0., 0., 0.],\n",
       "       ...,\n",
       "       [1., 0., 0., ..., 0., 0., 0.],\n",
       "       [1., 0., 1., ..., 0., 0., 0.],\n",
       "       [1., 1., 1., ..., 0., 0., 0.]])"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_gsc_sub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10000/10000 [11:21<00:00, 14.68it/s]  \n"
     ]
    }
   ],
   "source": [
    "W_trained_gsc_sub = LogisticRegression(X_train_gsc_sub,y_train_gsc_sub,iterate=10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Regression Loss On Training Data: 0.35709949920804535\n",
      "Loss On Training Data: 0.32449612369421893\n",
      "Accuracy On Training Data: 0.8888734062565318\n",
      "Loss On CV Data: 0.32802699333681967\n",
      "Accuracy On CV Data: 0.8861213683550477\n",
      "Loss On Test Data: 0.3262736664936357\n",
      "Accuracy On Test Data: 0.8868220294701641\n"
     ]
    }
   ],
   "source": [
    "train_loss,train_loss_logistic,train_accuracy = get_logisticRegression_metrics(X_train_gsc_sub,y_train_gsc_sub,W_trained_gsc_sub)\n",
    "cv_loss,cv_loss_logistic,cv_accuracy = get_logisticRegression_metrics(X_cv_gsc_sub,y_cv_gsc_sub,W_trained_gsc_sub)\n",
    "test_loss,test_Loss_logistic,test_accuracy = get_logisticRegression_metrics(X_test_gsc_sub,y_test_gsc_sub,W_trained_gsc_sub)\n",
    "\n",
    "print(f'Logistic Regression Loss On Training Data: {train_loss_logistic}')\n",
    "print(f'Loss On Training Data: {train_loss}\\nAccuracy On Training Data: {train_accuracy}')\n",
    "print(f'Loss On CV Data: {cv_loss}\\nAccuracy On CV Data: {cv_accuracy}')\n",
    "print(f'Loss On Test Data: {test_loss}\\nAccuracy On Test Data: {test_accuracy}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Linear Regression Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Concatenated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10, 1024)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|          | 111/10000 [00:00<00:08, 1102.78it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(86118, 10)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10000/10000 [00:09<00:00, 1105.68it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Accuracy: 0.5192294293875844\n",
      "CV Accuracy: 0.5176269769386191\n",
      "Test Accuracy: 0.5193855157278713\n",
      "E_rms TRAINING: 0.55057\n",
      "E_rms VALIDATION: 0.55195\n",
      "E_rms TEST: 0.55079\n"
     ]
    }
   ],
   "source": [
    "w_c_gsc = LinearRegression(X_train_gsc_con[:,1:],y_train_gsc_con,X_cv_gsc_con[:,1:],y_cv_gsc_con,X_test_gsc_con[:,1:],y_test_gsc_con,iterate=10000)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Subtracted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10, 512)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|▏         | 228/10000 [00:00<00:08, 1145.87it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(86118, 10)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10000/10000 [00:08<00:00, 1117.52it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Accuracy: 0.5545646670846978\n",
      "CV Accuracy: 0.5523583919738034\n",
      "Test Accuracy: 0.556588985264918\n",
      "E_rms TRAINING: 0.51973\n",
      "E_rms VALIDATION: 0.52044\n",
      "E_rms TEST: 0.51994\n"
     ]
    }
   ],
   "source": [
    "w_s_gsc = LinearRegression(X_train_gsc_sub[:,1:],y_train_gsc_sub,X_cv_gsc_sub[:,1:],y_cv_gsc_sub,X_test_gsc_sub[:,1:],y_test_gsc_sub,iterate=10000)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
